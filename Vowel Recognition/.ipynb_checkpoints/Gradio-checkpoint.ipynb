{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6e6e9f-bea4-454d-9e83-b1264bc1f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lip_recognition_1(image_in):\n",
    "    import joblib\n",
    "    import cv2\n",
    "    import mediapipe as mp\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    \n",
    "    rf_classifier = joblib.load('rf_classifier_a_op.pkl')\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    index_lips = [61, 76, 62, 78, \n",
    "                  185, 184, 183, 191, 95, 96, 77, 146, \n",
    "                  40, 74, 42, 80, 88, 89, 90, 91, \n",
    "                  39, 73, 41, 81, 178, 179, 180, 181, \n",
    "                  37, 72, 38, 82, 87, 86, 85, 84,\n",
    "                  0, 11, 12, 13, 14, 15, 16, 17,\n",
    "                  267, 302, 268, 312, 317, 316, 315, 314, \n",
    "                  269, 303, 271, 311, 402, 403, 404, 405, \n",
    "                  270, 304, 272, 310, 318, 319, 320, 321, \n",
    "                  409, 408, 407, 415, 324, 325, 307, 375, \n",
    "                  308, 292, 306, 291]\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "        lip_info = []\n",
    "        image = cv2.imread(image_in)\n",
    "        height, width, _ = image.shape\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        if results.multi_face_landmarks is not None:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for index in index_lips:\n",
    "                    #lip_info.append(face_landmarks.landmark[index])\n",
    "                    lip_info.append([face_landmarks.landmark[index].x, face_landmarks.landmark[index].y, face_landmarks.landmark[index].z])\n",
    "\n",
    "        pos_x = []\n",
    "        pos_y = []\n",
    "        pos_z = []\n",
    "        for i in lip_info:\n",
    "            pos_x.append(i[0])\n",
    "            pos_y.append(i[1])\n",
    "            pos_z.append(i[2])\n",
    "\n",
    "        aux_x = np.mean(pos_x)\n",
    "        aux_y = np.mean(pos_y)\n",
    "        aux_z = np.mean(pos_z)\n",
    "        lip_info.append([aux_x, aux_y, aux_z])\n",
    "\n",
    "    lip_info = np.array(lip_info)\n",
    "    coordenada_central = 80\n",
    "    img_lip_info_2 = []\n",
    "    for i in range(len(lip_info) - 1):\n",
    "        distancia = np.linalg.norm(lip_info[i] - lip_info[coordenada_central])\n",
    "        img_lip_info_2.append(distancia)\n",
    "\n",
    "    lip_info = pd.DataFrame(img_lip_info_2).transpose()\n",
    "    prediccion = rf_classifier.predict(lip_info)\n",
    "\n",
    "    switch = {\n",
    "        1: \"Vocal A\",\n",
    "        2: \"Vocal E\",\n",
    "        3: \"Vocal I\",\n",
    "        4: \"Vocal O\",\n",
    "        5: \"Vocal U\"\n",
    "    }\n",
    "    \n",
    "    return switch.get(prediccion[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ac8a32-318c-4851-ad55-6fede0a9c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lip_recognition_2(image_input):\n",
    "    import joblib\n",
    "    import cv2\n",
    "    import mediapipe as mp\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import io\n",
    "\n",
    "    # Cargar el modelo RandomForest previamente entrenado\n",
    "    rf_classifier = joblib.load('rf_classifier.pkl')\n",
    "    \n",
    "    # Inicializar el objeto de detección facial de MediaPipe\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    index_lips = [61, 76, 62, 78, \n",
    "                  185, 184, 183, 191, 95, 96, 77, 146, \n",
    "                  40, 74, 42, 80, 88, 89, 90, 91, \n",
    "                  39, 73, 41, 81, 178, 179, 180, 181, \n",
    "                  37, 72, 38, 82, 87, 86, 85, 84,\n",
    "                  0, 11, 12, 13, 14, 15, 16, 17,\n",
    "                  267, 302, 268, 312, 317, 316, 315, 314, \n",
    "                  269, 303, 271, 311, 402, 403, 404, 405, \n",
    "                  270, 304, 272, 310, 318, 319, 320, 321, \n",
    "                  409, 408, 407, 415, 324, 325, 307, 375, \n",
    "                  308, 292, 306, 291]\n",
    "\n",
    "    with open(image_input, \"rb\") as img_file:\n",
    "        image_in = img_file.read()\n",
    "\n",
    "    # Convertir la imagen de entrada en un formato adecuado\n",
    "    if isinstance(image_in, bytes):\n",
    "        # Si la imagen es en formato de bytes, conviértela en una imagen de PIL\n",
    "        image = Image.open(io.BytesIO(image_in))\n",
    "        # Guarda temporalmente la imagen en el sistema de archivos\n",
    "        temp_image_path = \"temp_image.jpg\"\n",
    "        image.save(temp_image_path)\n",
    "        # Carga la imagen con OpenCV\n",
    "        image = cv2.imread(temp_image_path)\n",
    "        # Borra la imagen temporal\n",
    "        os.remove(temp_image_path)\n",
    "    elif isinstance(image_in, str):\n",
    "        # Si la imagen es una cadena de caracteres, simplemente cárgala con OpenCV\n",
    "        image = cv2.imread(image_in)\n",
    "    else:\n",
    "        raise ValueError(\"El parámetro 'image_in' debe ser una cadena de caracteres (str) o una imagen en formato de bytes\")\n",
    "\n",
    "    # Procesar la imagen con MediaPipe Face Mesh\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "        lip_info = []\n",
    "        height, width, _ = image.shape\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        # Extraer las coordenadas de los puntos de los labios si se detectan caras en la imagen\n",
    "        if results.multi_face_landmarks is not None:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for index in index_lips:\n",
    "                    lip_info.append([face_landmarks.landmark[index].x, face_landmarks.landmark[index].y, face_landmarks.landmark[index].z])\n",
    "\n",
    "        # Calcular las coordenadas del punto medio de los labios\n",
    "        pos_x = [point[0] for point in lip_info]\n",
    "        pos_y = [point[1] for point in lip_info]\n",
    "        pos_z = [point[2] for point in lip_info]\n",
    "        mean_x = np.mean(pos_x)\n",
    "        mean_y = np.mean(pos_y)\n",
    "        mean_z = np.mean(pos_z)\n",
    "        lip_info.append([mean_x, mean_y, mean_z])\n",
    "\n",
    "    # Calcular la distancia de cada punto a un punto central\n",
    "    coordenada_central = 80\n",
    "    img_lip_info_2 = []\n",
    "    for i in range(len(lip_info) - 1):\n",
    "        distancia = np.linalg.norm(np.array(lip_info[i]) - np.array(lip_info[coordenada_central]))\n",
    "        img_lip_info_2.append(distancia)\n",
    "\n",
    "    # Convertir los datos de los labios en un DataFrame de Pandas\n",
    "    lip_info_df = pd.DataFrame([img_lip_info_2])\n",
    "\n",
    "    # Hacer una predicción utilizando el modelo RandomForest cargado\n",
    "    prediccion = rf_classifier.predict(lip_info_df)\n",
    "\n",
    "    # Mapear el resultado de la predicción a una vocal\n",
    "    switch = {\n",
    "        1: \"Vocal A\",\n",
    "        2: \"Vocal E\",\n",
    "        3: \"Vocal I\",\n",
    "        4: \"Vocal O\",\n",
    "        5: \"Vocal U\"\n",
    "    }\n",
    "\n",
    "    # Devolver la vocal predicha\n",
    "    return switch.get(prediccion[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8811e4ff-b955-461d-8c93-10e2470b5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal E\n"
     ]
    }
   ],
   "source": [
    "lip_info = lip_recognition_1('data/img/U/scene00001.jpg')\n",
    "print(lip_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d22ab88-3098-42b2-b040-8246299c62a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=lip_recognition_1,\n",
    "    inputs=gr.components.Image(height=300, width=300, type=\"filepath\", label=\"Input Image\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Clasificador de Vocales\",\n",
    "    description=\"Carga una imagen y el modelo predecirá su vocal.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efb8f887-0164-47e4-affd-8dd319a50951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cargar el modelo guardado\n",
    "loaded_model = load_model(\"vrdp.keras\", custom_objects={'softmax_v2': tf.nn.softmax})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec277bc4-d96d-47ec-8cee-81d3e12c667f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TensorFlowTrainer.predict of <Sequential name=sequential_9, built=True>>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c4d9125-7f84-4b24-8679-c6680ca958a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('df_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d559c08-1855-442c-8547-cc910de65020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.099192</td>\n",
       "      <td>0.092959</td>\n",
       "      <td>0.086554</td>\n",
       "      <td>0.083164</td>\n",
       "      <td>0.089762</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>0.075147</td>\n",
       "      <td>0.070456</td>\n",
       "      <td>0.06995</td>\n",
       "      <td>0.076266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070576</td>\n",
       "      <td>0.065389</td>\n",
       "      <td>0.072129</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.084359</td>\n",
       "      <td>0.090847</td>\n",
       "      <td>0.0807</td>\n",
       "      <td>0.084646</td>\n",
       "      <td>0.09149</td>\n",
       "      <td>0.098096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.099192  0.092959  0.086554  0.083164  0.089762  0.082877  0.075147   \n",
       "\n",
       "          7        8         9  ...        70        71        72        73  \\\n",
       "0  0.070456  0.06995  0.076266  ...  0.070576  0.065389  0.072129  0.078125   \n",
       "\n",
       "         74        75      76        77       78        79  \n",
       "0  0.084359  0.090847  0.0807  0.084646  0.09149  0.098096  \n",
       "\n",
       "[1 rows x 80 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.iloc[0]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25a1a75f-0c84-4947-9f72-4bb5b5961123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n"
     ]
    }
   ],
   "source": [
    "prediccion = loaded_model.predict(pd.DataFrame(df.iloc[0]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad141618-4ea9-494e-a34d-f5a0917ec6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['A', 'E', 'I', 'O', 'U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5288f814-b4e0-431c-900a-f545bc375903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lip_r_dl(image_in):\n",
    "    import cv2\n",
    "    import mediapipe as mp\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    from keras.models import load_model\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    vrdp_classifier = load_model(\"vrdp.keras\", custom_objects={'softmax_v2': tf.nn.softmax})\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    index_lips = [61, 76, 62, 78, \n",
    "                  185, 184, 183, 191, 95, 96, 77, 146, \n",
    "                  40, 74, 42, 80, 88, 89, 90, 91, \n",
    "                  39, 73, 41, 81, 178, 179, 180, 181, \n",
    "                  37, 72, 38, 82, 87, 86, 85, 84,\n",
    "                  0, 11, 12, 13, 14, 15, 16, 17,\n",
    "                  267, 302, 268, 312, 317, 316, 315, 314, \n",
    "                  269, 303, 271, 311, 402, 403, 404, 405, \n",
    "                  270, 304, 272, 310, 318, 319, 320, 321, \n",
    "                  409, 408, 407, 415, 324, 325, 307, 375, \n",
    "                  308, 292, 306, 291]\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "        lip_info = []\n",
    "        image = cv2.imread(image_in)\n",
    "        height, width, _ = image.shape\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        if results.multi_face_landmarks is not None:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for index in index_lips:\n",
    "                    #lip_info.append(face_landmarks.landmark[index])\n",
    "                    lip_info.append([face_landmarks.landmark[index].x, face_landmarks.landmark[index].y, face_landmarks.landmark[index].z])\n",
    "\n",
    "        pos_x = []\n",
    "        pos_y = []\n",
    "        pos_z = []\n",
    "        for i in lip_info:\n",
    "            pos_x.append(i[0])\n",
    "            pos_y.append(i[1])\n",
    "            pos_z.append(i[2])\n",
    "\n",
    "        aux_x = np.mean(pos_x)\n",
    "        aux_y = np.mean(pos_y)\n",
    "        aux_z = np.mean(pos_z)\n",
    "        lip_info.append([aux_x, aux_y, aux_z])\n",
    "\n",
    "    lip_info = np.array(lip_info)\n",
    "    coordenada_central = 80\n",
    "    img_lip_info_2 = []\n",
    "    for i in range(len(lip_info) - 1):\n",
    "        distancia = np.linalg.norm(lip_info[i] - lip_info[coordenada_central])\n",
    "        img_lip_info_2.append(distancia)\n",
    "\n",
    "    lip_info = pd.DataFrame(img_lip_info_2).transpose()\n",
    "    prediccion = vrdp_classifier.predict(lip_info)\n",
    "\n",
    "    #vocales = ['A', 'E', 'I', 'O', 'U']\n",
    "    #posicion_maxima = np.argmax(np.array(prediccion))\n",
    "    #vocal_maxima = vocales[posicion_maxima]\n",
    "    \n",
    "    return prediccion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9cf03289-bd5b-490a-b681-77ca2f825bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javit\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 14 variables whereas the saved optimizer has 26 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "[[0.7835485  0.0787283  0.01461062 0.11100323 0.01210934]]\n"
     ]
    }
   ],
   "source": [
    "lip_info = lip_r_dl('data/img/I/scene00001.jpg')\n",
    "print(lip_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38e7eef8-91f6-4ab5-ac80-c750c8cf4336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocales = ['A', 'E', 'I', 'O', 'U']\n",
    "# Encontrar la posición del valor más alto en el array de entrada\n",
    "posicion_maxima = np.argmax(np.array(lip_info))\n",
    "\n",
    "# Obtener la vocal correspondiente a la posición máxima\n",
    "vocal_maxima = vocales[posicion_maxima]\n",
    "\n",
    "# Output\n",
    "print(vocal_maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04ec769f-0bd1-469e-8366-1715e58cd40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=lip_r_dl,\n",
    "    inputs=gr.components.Image(type=\"filepath\", label=\"Input Image\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Clasificador de Vocales\",\n",
    "    description=\"Carga una imagen y el modelo predecirá su clase.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5dbf2f-0a0d-4207-a777-5ab7f935582f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c90559-2214-40eb-a289-553953eed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd2d620-8766-44be-9c8b-59a807e9045c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/img')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_url = \"data/img\"\n",
    "data_dir = pathlib.Path(dataset_url)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2695f443-8000-43ea-80ab-2652597d1484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\img\\E\\scene00001.jpg\n"
     ]
    }
   ],
   "source": [
    "E = list(data_dir.glob('E/*'))\n",
    "print(E[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b2ccc8-885a-400f-b880-da071eb3f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 717 files belonging to 5 classes.\n",
      "Using 574 files for training.\n"
     ]
    }
   ],
   "source": [
    "img_height,img_width=180,180\n",
    "batch_size=32\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a2d486-52f3-49a4-9dbd-104287a7e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 717 files belonging to 5 classes.\n",
      "Using 143 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ec56729-e07c-4385-a028-6257d82bbf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'E', 'I', 'O', 'U']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "742b7c64-7fbb-41c2-9198-ebf9b0d2ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "model = Sequential([\n",
    "  keras.Input(shape=(180, 180, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "228c99ef-7000-4569-8acd-b70cedc48853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import sparse_categorical_crossentropy\n",
    "model.compile(optimizer='adam',\n",
    "              loss=sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a74952d-22b1-4427-ac9d-cce0840be9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.2293 - loss: 127.1779 - val_accuracy: 0.4406 - val_loss: 1.4168\n",
      "Epoch 2/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 170ms/step - accuracy: 0.5470 - loss: 1.1805 - val_accuracy: 0.5944 - val_loss: 0.9134\n",
      "Epoch 3/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 171ms/step - accuracy: 0.8192 - loss: 0.5368 - val_accuracy: 0.6923 - val_loss: 0.8747\n",
      "Epoch 4/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 170ms/step - accuracy: 0.8972 - loss: 0.3022 - val_accuracy: 0.8462 - val_loss: 0.3669\n",
      "Epoch 5/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 173ms/step - accuracy: 0.9955 - loss: 0.0586 - val_accuracy: 0.9371 - val_loss: 0.1843\n",
      "Epoch 6/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 171ms/step - accuracy: 0.9914 - loss: 0.0372 - val_accuracy: 0.9441 - val_loss: 0.1600\n",
      "Epoch 7/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - accuracy: 0.9983 - loss: 0.0137 - val_accuracy: 0.9301 - val_loss: 0.2347\n",
      "Epoch 8/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 170ms/step - accuracy: 0.9969 - loss: 0.0108 - val_accuracy: 0.9301 - val_loss: 0.1642\n",
      "Epoch 9/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.9939 - loss: 0.0265 - val_accuracy: 0.9441 - val_loss: 0.2273\n",
      "Epoch 10/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.9947 - loss: 0.0187 - val_accuracy: 0.9441 - val_loss: 0.2101\n",
      "Epoch 11/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 0.9510 - val_loss: 0.1519\n",
      "Epoch 12/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 9.9480e-04 - val_accuracy: 0.9580 - val_loss: 0.1228\n",
      "Epoch 13/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 3.0914e-04 - val_accuracy: 0.9650 - val_loss: 0.0875\n",
      "Epoch 14/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 1.8779e-04 - val_accuracy: 0.9650 - val_loss: 0.0961\n",
      "Epoch 15/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 1.4651e-04 - val_accuracy: 0.9650 - val_loss: 0.0967\n",
      "Epoch 16/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 1.4397e-04 - val_accuracy: 0.9650 - val_loss: 0.0972\n",
      "Epoch 17/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 1.0289e-04 - val_accuracy: 0.9650 - val_loss: 0.0973\n",
      "Epoch 18/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 8.6847e-05 - val_accuracy: 0.9650 - val_loss: 0.0921\n",
      "Epoch 19/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 8.7705e-05 - val_accuracy: 0.9650 - val_loss: 0.0915\n",
      "Epoch 20/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 7.7266e-05 - val_accuracy: 0.9650 - val_loss: 0.0929\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239780a0-2450-4b83-9a92-26ce0e11dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img):\n",
    "    import cv2\n",
    "    img = cv2.imread(img)\n",
    "    imagen_redimensionada = cv2.resize(img, (180, 180))\n",
    "    imagen_redimensionada_expandida = np.expand_dims(imagen_redimensionada, axis=0)\n",
    "    #img_4d=img.reshape(-1,180,180,3)\n",
    "    prediction=model.predict(imagen_redimensionada_expandida)[0]\n",
    "    return {class_names[i]: float(prediction[i]) for i in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb367177-866b-4a9e-90f9-6261b19ad4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(fn=predict_image, inputs=gr.Image(height=180, width=180, type=\"filepath\", label=\"Input Image\"), outputs=gr.Label(num_top_classes=5)).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed47d26d-2e70-47ec-8b98-1ae4143d71ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 6.749868771294132e-05\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(train_ds, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9328deab-c143-40c6-987c-85112c5b6e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09293024986982346\n",
      "Test accuracy: 0.9650349617004395\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(val_ds, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881ccdf-cf7d-4198-a10d-2c9b184462c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
