{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ea62af-e7e8-41f8-ba62-a8010509545d",
   "metadata": {},
   "source": [
    "### Llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645a3abc-3f54-430a-801d-a01a9febfa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4b35d-2e6a-441f-98d2-6feb260459a6",
   "metadata": {},
   "source": [
    "### Funcions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373666ec-8ed7-424b-8d6e-5c88077d0f56",
   "metadata": {},
   "source": [
    "Inicialitzem i guardem les funcions per fer el face mesh.\r\n",
    "El 'index_lips' són tots els punts que defineixen els llavis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1c89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "index_lips = [61, 76, 62, 78, \n",
    "              185, 184, 183, 191, 95, 96, 77, 146, \n",
    "              40, 74, 42, 80, 88, 89, 90, 91, \n",
    "              39, 73, 41, 81, 178, 179, 180, 181, \n",
    "              37, 72, 38, 82, 87, 86, 85, 84,\n",
    "              0, 11, 12, 13, 14, 15, 16, 17,\n",
    "              267, 302, 268, 312, 317, 316, 315, 314, \n",
    "              269, 303, 271, 311, 402, 403, 404, 405, \n",
    "              270, 304, 272, 310, 318, 319, 320, 321, \n",
    "              409, 408, 407, 415, 324, 325, 307, 375, \n",
    "              308, 292, 306, 291]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadae7af-946b-4166-8231-95647894ec34",
   "metadata": {},
   "source": [
    "### Visualitzar tots els punts i els contorns de 'face_mesh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae04a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    image = cv2.imread('data/img/A_3/scene00126.jpg')\n",
    "    height, width, _ = image.shape\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    if results.multi_face_landmarks is not None:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, face_landmarks,\n",
    "                        mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                        mp_drawing.DrawingSpec(color=(0,255,0), thickness=6, circle_radius=1),\n",
    "                        mp_drawing.DrawingSpec(color=(0,255,0), thickness=6))\n",
    "\n",
    "    image_resize = cv2.resize(image, (544, 960))    \n",
    "    cv2.imwrite('data/face_mesh.jpg', image)\n",
    "    cv2.imshow(\"Image\", image_resize)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96388260-fb1d-47f3-a686-7767f7b47736",
   "metadata": {},
   "source": [
    "### Visualitzar tots els punts de 'index_lips'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c90e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_info = []\n",
    "lip_info_2 = []\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    image = cv2.imread('data/img/A_3/scene00126.jpg')\n",
    "    height, width, _ = image.shape\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    if results.multi_face_landmarks is not None:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for index in index_lips:\n",
    "                lip_info.append([face_landmarks.landmark[index].x, face_landmarks.landmark[index].y, face_landmarks.landmark[index].z])\n",
    "                lip_info_2.append([face_landmarks.landmark[index].x, face_landmarks.landmark[index].y])\n",
    "                x = int(face_landmarks.landmark[index].x * width)\n",
    "                y = int(face_landmarks.landmark[index].y * height)\n",
    "                cv2.circle(image, (x, y), 2, (255, 0, 255), 12)    \n",
    "                aux_l = face_landmarks.landmark[index]\n",
    "                \n",
    "    pos_x = []\n",
    "    pos_y = []\n",
    "    pos_z = []\n",
    "    for i in lip_info:\n",
    "        pos_x.append(i[0])\n",
    "        pos_y.append(i[1])\n",
    "        pos_z.append(i[2])\n",
    "    aux_x = np.mean(pos_x)\n",
    "    aux_y = np.mean(pos_y)\n",
    "    aux_z = np.mean(pos_z)\n",
    "    lip_info.append([aux_x, aux_y, aux_z])\n",
    "\n",
    "    pos_x = []\n",
    "    pos_y = []\n",
    "    for i in lip_info_2:\n",
    "        pos_x.append(i[0])\n",
    "        pos_y.append(i[1])\n",
    "    aux_x = np.mean(pos_x)\n",
    "    aux_y = np.mean(pos_y)\n",
    "    lip_info_2.append([aux_x, aux_y])\n",
    "\n",
    "    image_resize = cv2.resize(image, (544, 960))\n",
    "    cv2.imshow(\"Image\", image_resize)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f38b4-05e3-4495-bf22-2944e9783560",
   "metadata": {},
   "source": [
    "### Visualitzar el requadre que conté el rostre amb 'FaceDetection'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a7d37-b2f5-4362-a2b1-9efb6fead52a",
   "metadata": {},
   "source": [
    "Aquesta part pertany a les proves que van ser necessàries per fer el preprocessament del model que prediu paraules en angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3e8453-fbb2-42d8-85e5-414d9daf5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_info = []\n",
    "lip_info_2 = []\n",
    "# Inicializar los objetos de los modelos\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Cargar la imagen\n",
    "image_path = \"data/img/A_3/scene00126.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "ih, iw, _ = image.shape\n",
    "\n",
    "# Convertir la imagen a RGB\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Realizar la detección de caras\n",
    "results_detection = face_detection.process(image_rgb)\n",
    "\n",
    "# Realizar el seguimiento de los puntos faciales si se detecta una cara\n",
    "if results_detection.detections:\n",
    "    for detection in results_detection.detections:\n",
    "        bboxC = detection.location_data.relative_bounding_box\n",
    "        xmin = int(bboxC.xmin * iw)\n",
    "        ymin = int(bboxC.ymin * ih)\n",
    "        w = int(bboxC.width * iw)\n",
    "        h = int(bboxC.height * ih)\n",
    "        xmax = xmin + w\n",
    "        ymax = ymin + h\n",
    "        bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "               int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "        # Dibujar el cuadro delimitador de la cara\n",
    "        cv2.rectangle(image, bbox, (0, 255, 0), 3)\n",
    "        \n",
    "        # Realizar el seguimiento de los puntos faciales\n",
    "        results_mesh = face_mesh.process(image_rgb)\n",
    "        \n",
    "        # Dibujar los puntos faciales si se detectan\n",
    "        if results_mesh.multi_face_landmarks:\n",
    "            for face_landmarks in results_mesh.multi_face_landmarks:\n",
    "                for index in index_lips:\n",
    "                    lip_info.append([face_landmarks.landmark[index].x, face_landmarks.landmark[index].y, face_landmarks.landmark[index].z])\n",
    "                    lip_info_2.append([face_landmarks.landmark[index].x, face_landmarks.landmark[index].y])\n",
    "                    x = int(face_landmarks.landmark[index].x * iw)\n",
    "                    y = int(face_landmarks.landmark[index].y * ih)\n",
    "                    cv2.circle(image, (x, y), 2, (255, 0, 255), 12)\n",
    "                    aux_l = face_landmarks.landmark[index]\n",
    "                \n",
    "    pos_x = []\n",
    "    pos_y = []\n",
    "    pos_z = []\n",
    "    for i in lip_info:\n",
    "        pos_x.append(i[0])\n",
    "        pos_y.append(i[1])\n",
    "        pos_z.append(i[2])\n",
    "    aux_x = np.mean(pos_x)\n",
    "    aux_y = np.mean(pos_y)\n",
    "    aux_z = np.mean(pos_z)\n",
    "    lip_info.append([aux_x, aux_y, aux_z])\n",
    "\n",
    "    pos_x = []\n",
    "    pos_y = []\n",
    "    for i in lip_info_2:\n",
    "        pos_x.append(i[0])\n",
    "        pos_y.append(i[1])\n",
    "    aux_x = np.mean(pos_x)\n",
    "    aux_y = np.mean(pos_y)\n",
    "    lip_info_2.append([aux_x, aux_y])      \n",
    "    image_resize = cv2.resize(image, (544, 960))\n",
    "    cv2.imshow(\"Image\", image_resize)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fae8dae-3cb2-4865-958d-709f860aa03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proyección ortográfica de los puntos del labio en el plano de la cara\n",
    "face_center_x = (xmin + xmax) // 2\n",
    "face_center_y = (ymin + ymax) // 2\n",
    "face_center_z = 0  # Suponemos que la cara está en el mismo plano Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e96bf2c4-44c5-43d5-9909-137b659f0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_lip_points = []\n",
    "for point in lip_info:\n",
    "    # Aplicar la proyección ortográfica\n",
    "    projected_x = (point[0]*iw) - face_center_x\n",
    "    projected_y = (point[1]*ih) - face_center_y\n",
    "    projected_z = point[2] - face_center_z\n",
    "    projected_lip_points.append([projected_x, projected_y, projected_z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db52309d-dff6-42b2-8c9a-cd8c2609a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las distancias euclidianas\n",
    "distances = []\n",
    "for projected_point in projected_lip_points:\n",
    "    distance = np.linalg.norm(projected_point)\n",
    "    distances.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0b6618-eb1b-4b46-b575-cc64e84e6349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[211.98847000809292,\n",
       " 206.87840504223834,\n",
       " 200.3695157103275,\n",
       " 194.93225206259385,\n",
       " 179.03045161722855,\n",
       " 177.25566484537478,\n",
       " 173.85082096424904,\n",
       " 164.6996701635703,\n",
       " 216.9487249276778,\n",
       " 222.86868843481324,\n",
       " 229.81799713925824,\n",
       " 237.1042667090333,\n",
       " 148.60721864405826,\n",
       " 149.6977432692294,\n",
       " 151.81515609987997,\n",
       " 146.75520018462421,\n",
       " 229.03774751145912,\n",
       " 237.56382517036482,\n",
       " 251.5699266171585,\n",
       " 264.99002421215016,\n",
       " 113.23458375933961,\n",
       " 124.19355563513217,\n",
       " 133.04970904184512,\n",
       " 133.03683931785622,\n",
       " 240.71067701373133,\n",
       " 254.48469379804925,\n",
       " 274.1513511212464,\n",
       " 295.0180915204672,\n",
       " 79.91323477662009,\n",
       " 103.36105935828152,\n",
       " 121.00843053434699,\n",
       " 126.82025862690543,\n",
       " 250.4951474864787,\n",
       " 268.9000406392005,\n",
       " 293.81655619566965,\n",
       " 316.32938648317594,\n",
       " 93.01034311529592,\n",
       " 111.28073089323615,\n",
       " 125.75001837573794,\n",
       " 131.9715054303722,\n",
       " 258.4013491953956,\n",
       " 279.82493491590384,\n",
       " 303.49314307812006,\n",
       " 327.1930237871127,\n",
       " 117.62320513947834,\n",
       " 132.56325637442004,\n",
       " 143.39510167597143,\n",
       " 145.13479890003106,\n",
       " 262.1061206589085,\n",
       " 281.0955764915363,\n",
       " 306.48926605796044,\n",
       " 328.7265693466039,\n",
       " 162.68782387521782,\n",
       " 165.1872585081771,\n",
       " 166.86280070393988,\n",
       " 162.51751022736497,\n",
       " 260.253753939149,\n",
       " 275.3371267692568,\n",
       " 296.0101949356873,\n",
       " 316.9077569410403,\n",
       " 195.43083729505423,\n",
       " 192.2460536815495,\n",
       " 188.59709928694818,\n",
       " 180.79178851915327,\n",
       " 254.07955261482627,\n",
       " 264.1073673055823,\n",
       " 279.75642765717345,\n",
       " 293.81902752431756,\n",
       " 222.0975730966537,\n",
       " 217.76496405882102,\n",
       " 211.20493546561858,\n",
       " 200.20610779956158,\n",
       " 245.9121921701177,\n",
       " 254.43304840663643,\n",
       " 262.91073095522285,\n",
       " 271.1798964224013,\n",
       " 229.22451092107733,\n",
       " 235.91759246432193,\n",
       " 244.09178100778885,\n",
       " 250.34442108256047,\n",
       " 186.14622888815651]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2718f9e-701c-4aa7-95a7-c9cda474a12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5362813234329223, 0.6204870626330375, -0.03463999899668124]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lip_info[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cb8e51-4be9-4507-acdc-e09528585b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.384128600358963, 0.6135057210922241]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lip_info_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e08737-f101-4221-a84c-b5d2c17ba7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocales_info_np = np.array(lip_info)\n",
    "vocales_info_np_2 = np.array(lip_info_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1bd8d32-3b0a-40fb-b776-2d8cf0703ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocales_info_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7cbf0e9-593b-4a03-8456-749d413f7cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1580784873779008"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distancia = np.linalg.norm(vocales_info_np[0] - vocales_info_np[80])\n",
    "distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f24e9ca-8be4-475c-bbe8-5b749e4ba79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53628132,  0.62048706, -0.03464   ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocales_info_np[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d9c6d08-84c0-4b5c-9ac0-aff882394c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordenada_central = 80\n",
    "img_lip_info_1 = []\n",
    "for i in range(len(vocales_info_np)):\n",
    "    distancia = np.linalg.norm(vocales_info_np[i] - vocales_info_np[coordenada_central])\n",
    "    img_lip_info_1.append(distancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6023491f-1a93-4df3-84c8-d22db7336340",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordenada_central = 80\n",
    "img_lip_info_2= []\n",
    "for i in range(len(vocales_info_np_2)):\n",
    "    distancia = np.linalg.norm(vocales_info_np_2[i] - vocales_info_np_2[coordenada_central])\n",
    "    img_lip_info_2.append(distancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd2593-52db-4865-8e42-394e8d6c0870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
